import os
from medperf.tests.mocks.cube import TestCube
import yaml
import logging
from time import time
from pathlib import Path
from typing import List

import medperf.config as config
from medperf.commands.execution import Execution
from medperf.entities.cube import Cube
from medperf.entities.dataset import Dataset
from medperf.entities.benchmark import Benchmark
from medperf.entities.report import TestReport
from medperf.commands.dataset.create import DataPreparation
from medperf.utils import check_cube_validity, untar, get_file_sha1, storage_path
from medperf.exceptions import InvalidArgumentError, InvalidEntityError
from medperf.comms.entity_resources import resources
from .validate_params import CompatibilityTestParamsValidator


class CompatibilityTestExecution(CompatibilityTestParamsValidator):
    @classmethod
    def run(
        cls,
        model: str = None,
        evaluator: str = None,
        benchmark: int = None,
        data_prep: str = None,
        data_path: str = None,
        labels_path: str = None,
        demo_dataset_url: str = None,
        demo_dataset_hash: str = None,
        data_uid: str = None,
        no_cache: bool = False,
    ) -> List:
        """Execute a test workflow. Components of a complete workflow should be passed.
        When only the benchmark is provided, it implies the following workflow will be used:
        - the benchmark's demo dataset is used as the raw data
        - the benchmark's data preparation cube is used
        - the benchmark's reference model cube is used
        - the benchmark's metrics cube is used

        Overriding benchmark's components:
        - The data prepration, model, and metrics cubes can be overriden by specifying a cube either
        as an integer (registered) or a path (local)
        - The input raw data can be overriden by providing:
            - a demo dataset url
            - data path and labels path
        - A prepared dataset can be directly used. In this case the data preparator cube is never used.

        Whether the benchmark is provided or not, the command will fail either if the user fails to
        provide a valid complete workflow, or if the user provided extra redundant parameters.


        Args:
            benchmark_uid (int, str): Benchmark to run the test workflow for.
                Either a server uid or an unregistered benchmark generated uid
            data_uid (str, optional): registered dataset uid.
                If none provided, it defaults to benchmark test dataset.
            data_prep (str, optional): data prep mlcube uid or local path.
                If none provided, it defaults to benchmark data prep mlcube.
            model (str, optional): model mlcube uid or local path.
                If none provided, it defaults to benchmark reference model.
            evaluator (str, optional): evaluator mlcube uid or local path.
                If none provided, it defaults to benchmark evaluator mlcube.
        Returns:
            (str): Benchmark UID used for the test. Could be the one provided or a generated one.
            (str): Dataset UID used for the test. Could be the one provided or a generated one.
            (str): Model UID used for the test. Could be the one provided or a generated one.
            (Result): Results generated by the test.
        """
        logging.info("Starting test execution")
        test_exec = cls(
            model,
            evaluator,
            benchmark,
            data_prep,
            data_path,
            labels_path,
            demo_dataset_url,
            demo_dataset_hash,
            data_uid,
            no_cache,
        )
        test_exec.validate()
        test_exec.prepare_test()
        test_exec.set_data_uid()
        test_exec.initialize_report()
        results = test_exec.cached_results()
        if results is None:
            results = test_exec.execute_benchmark()
            test_exec.write(results)
        return test_exec.benchmark_uid, test_exec.data_uid, test_exec.model, results

    def __init__(
        self,
        model: str = None,
        evaluator: str = None,
        benchmark: int = None,
        data_prep: str = None,
        data_path: str = None,
        labels_path: str = None,
        demo_dataset_url: str = None,
        demo_dataset_hash: str = None,
        data_uid: str = None,
        no_cache: bool = False,
    ):
        self.model = model
        self.evaluator = evaluator
        self.benchmark_uid = benchmark
        self.data_prep = data_prep
        self.data_path = data_path
        self.labels_path = labels_path
        self.demo_dataset_url = demo_dataset_url
        self.demo_dataset_hash = demo_dataset_hash
        self.data_uid = data_uid
        self.comms = config.comms
        self.ui = config.ui
        self.dataset = None
        self.no_cache = no_cache
        self.from_benchmark = False
        self.from_prepared = False
        self.from_path = False
        self.from_demo = False

    def prepare_test(self):
        """Prepares all parameters so a test can be executed. Paths to cubes are
        transformed to cube uids and benchmark is mocked/obtained.
        """
        if self.benchmark_uid:
            self.benchmark = Benchmark.get(self.benchmark_uid)
            self.set_cube_uid("data_prep", self.benchmark.data_preparation_mlcube)
            self.set_cube_uid("model", self.benchmark.reference_model_mlcube)
            self.set_cube_uid("evaluator", self.benchmark.data_evaluator_mlcube)
        else:
            self.set_cube_uid("data_prep")
            self.set_cube_uid("model")
            self.set_cube_uid("evaluator")

        self.model_cube = self.__get_cube(self.model, "Model")
        self.evaluator_cube = self.__get_cube(self.evaluator, "Evaluator")

    def __get_cube(self, uid: int, name: str) -> Cube:
        self.ui.text = f"Retrieving {name} cube"
        cube = Cube.get(uid)
        self.ui.print(f"> {name} cube download complete")
        check_cube_validity(cube)
        return cube

    def execute_benchmark(self):
        """Runs the benchmark execution flow given the specified testing parameters
        """
        execution_summary = Execution.run(
            dataset=self.dataset,
            model=self.model_cube,
            evaluator=self.evaluator_cube,
            ignore_model_errors=False,
        )
        return execution_summary["results"]

    def set_cube_uid(self, attr: str, fallback: any = None):
        """Assigns the attr used for testing according to the initialization parameters.
        If the value is a path, it will create a temporary uid and link the cube path to
        the medperf storage path.

        Arguments:
            attr (str): Attribute to check and/or reassign.
            fallback (any): Value to assign if attribute is empty. Defaults to None.
        """
        logging.info(f"Establishing {attr}_uid for test execution")
        val = getattr(self, attr)
        if val is None:
            logging.info(f"Empty attribute: {attr}. Assigning fallback: {fallback}")
            setattr(self, attr, fallback)
            return

        # Test if value looks like an mlcube_uid, if so skip path validation
        if str(val).isdigit():
            logging.info(f"MLCube value {val} for {attr} resembles an mlcube_uid")
            return

        # Check if value is a local mlcube
        path = Path(val)
        if path.is_file():
            path = path.parent
        path = path.resolve()

        if os.path.exists(path):
            logging.info("local path provided. Creating symbolic link")
            temp_uid = config.test_cube_prefix + str(int(time()))
            setattr(self, attr, temp_uid)
            self.prepare_local_cube(path, temp_uid)
            return

        logging.error(f"mlcube {val} was not found as an existing mlcube")
        raise InvalidArgumentError(
            f"The provided mlcube ({val}) for {attr} could not be found as a local or remote mlcube"
        )

    def prepare_local_cube(self, path, temp_uid):
        cubes_storage = storage_path(config.cubes_storage)
        dst = os.path.join(cubes_storage, temp_uid)
        os.symlink(path, dst)
        logging.info(f"local cube will be linked to path: {dst}")
        cube_metadata_file = os.path.join(path, config.cube_metadata_filename)
        cube_hashes_filename = os.path.join(path, config.cube_hashes_filename)
        if not os.path.exists(cube_metadata_file):
            temp_metadata = {
                "id": temp_uid,
                "name": temp_uid,
                "mlcube_hash": "",
                "parameters_hash": "",
                "image_tarball_hash": "",
                "additional_files_tarball_hash": "",
            }
            metadata = TestCube(**temp_metadata).todict()
            with open(cube_metadata_file, "w") as f:
                yaml.dump(metadata, f)
        if not os.path.exists(cube_hashes_filename):
            hashes = {
                "mlcube_hash": "",
                "parameters_hash": "",
                "additional_files_tarball_hash": "",
                "image_tarball_hash": "",
            }
            with open(cube_hashes_filename, "w") as f:
                yaml.dump(hashes, f)

    def set_data_uid(self):
        """Assigns the data_uid used for testing according to the initialization parameters.
        If no data_uid is provided, it will retrieve the demo data and execute the data
        preparation flow.
        """
        logging.info("Establishing data_uid for test execution")
        logging.info("Looking if dataset exists as a prepared dataset")
        if self.from_prepared:
            self.dataset = Dataset.get(self.data_uid)
            # to avoid 'None' as a uid
            self.data_prep = self.dataset.data_preparation_mlcube
        else:
            if self.from_path:
                data_path, labels_path = self.data_path, self.labels_path
            else:
                if self.from_benchmark:
                    logging.info("Using benchmark demo dataset")
                    self.demo_dataset_url = self.benchmark.demo_dataset_tarball_url
                    self.demo_dataset_hash = self.benchmark.demo_dataset_tarball_hash
                data_path, labels_path = self.download_demo_data()

            self.data_uid = DataPreparation.run(
                None,
                self.data_prep,
                data_path,
                labels_path,
                run_test=True,
                name="demo_data",
                location="local",
            )
            self.dataset = Dataset.get(self.data_uid)

    def download_demo_data(self):
        """Retrieves the demo dataset associated to the specified benchmark

        Returns:
            data_path (str): Location of the downloaded data
            labels_path (str): Location of the downloaded labels
        """
        dset_hash = self.demo_dataset_hash
        dset_url = self.demo_dataset_url
        file_path = resources.get_benchmark_demo_dataset(dset_url, dset_hash)

        # Check demo dataset integrity
        file_hash = get_file_sha1(file_path)
        # Alllow for empty datset hashes for benchmark registration purposes
        if dset_hash and file_hash != dset_hash:
            raise InvalidEntityError("Demo dataset hash doesn't match expected hash")

        untar_path = untar(file_path, remove=False)

        # It is assumed that all demo datasets contain a file
        # which specifies the input of the data preparation step
        paths_file = os.path.join(untar_path, config.demo_dset_paths_file)
        with open(paths_file, "r") as f:
            paths = yaml.safe_load(f)

        data_path = os.path.join(untar_path, paths["data_path"])
        labels_path = os.path.join(untar_path, paths["labels_path"])
        return data_path, labels_path

    def initialize_report(self):
        report_data = {
            "demo_dataset_url": self.demo_dataset_url,
            "demo_dataset_hash": self.demo_dataset_hash,
            "data_path": self.data_path,
            "labels_path": self.labels_path,
            "prepared_data_hash": self.dataset.generated_uid,
            "data_preparation_mlcube": self.data_prep,
            "model": self.model,
            "data_evaluator_mlcube": self.evaluator,
        }
        self.report = TestReport(**report_data)

    def cached_results(self):
        """checks the existance of, and retrieves if possible, the compatibility test
        result. This method is called prior to the test execution.

        Returns:
            (dict|None): None if the result does not exist or if self.no_cache is True,
            otherwise it returns the found result.
        """
        if not self.no_cache:
            return self.report.cached_results()

    def write(self, results):
        self.report.set_results(results)
        self.report.write()
