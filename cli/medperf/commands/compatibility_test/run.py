import logging
from typing import List

from medperf.commands.execution import Execution
from medperf.entities.dataset import Dataset
from medperf.entities.benchmark import Benchmark
from medperf.entities.report import TestReport
from medperf.commands.dataset.create import DataPreparation
from .validate_params import CompatibilityTestParamsValidator
from .utils import download_demo_data, prepare_cube, get_cube


class CompatibilityTestExecution(CompatibilityTestParamsValidator):
    @classmethod
    def run(
        cls,
        benchmark: int = None,
        data_prep: str = None,
        model: str = None,
        evaluator: str = None,
        data_path: str = None,
        labels_path: str = None,
        demo_dataset_url: str = None,
        demo_dataset_hash: str = None,
        data_uid: str = None,
        no_cache: bool = False,
        offline: bool = False,
    ) -> List:
        """Execute a test workflow. Components of a complete workflow should be passed.
        When only the benchmark is provided, it implies the following workflow will be used:
        - the benchmark's demo dataset is used as the raw data
        - the benchmark's data preparation cube is used
        - the benchmark's reference model cube is used
        - the benchmark's metrics cube is used

        Overriding benchmark's components:
        - The data prepration, model, and metrics cubes can be overriden by specifying a cube either
        as an integer (registered) or a path (local)
        - The input raw data can be overriden by providing:
            - a demo dataset url
            - data path and labels path
        - A prepared dataset can be directly used. In this case the data preparator cube is never used.

        Whether the benchmark is provided or not, the command will fail either if the user fails to
        provide a valid complete workflow, or if the user provided extra redundant parameters.


        Args:
            benchmark_uid (int, str): Benchmark to run the test workflow for.
                Either a server uid or an unregistered benchmark generated uid
            data_uid (str, optional): registered dataset uid.
                If none provided, it defaults to benchmark test dataset.
            data_prep (str, optional): data prep mlcube uid or local path.
                If none provided, it defaults to benchmark data prep mlcube.
            model (str, optional): model mlcube uid or local path.
                If none provided, it defaults to benchmark reference model.
            evaluator (str, optional): evaluator mlcube uid or local path.
                If none provided, it defaults to benchmark evaluator mlcube.
        Returns:
            (str): Benchmark UID used for the test. Could be the one provided or a generated one.
            (str): Dataset UID used for the test. Could be the one provided or a generated one.
            (str): Model UID used for the test. Could be the one provided or a generated one.
            (Result): Results generated by the test.
        """
        logging.info("Starting test execution")
        test_exec = cls(
            benchmark,
            data_prep,
            model,
            evaluator,
            data_path,
            labels_path,
            demo_dataset_url,
            demo_dataset_hash,
            data_uid,
            no_cache,
            offline,
        )
        test_exec.validate()
        test_exec.process_benchmark()
        test_exec.prepare_cubes()
        test_exec.prepare_dataset()
        test_exec.initialize_report()
        results = test_exec.cached_results()
        if results is None:
            results = test_exec.execute()
            test_exec.write(results)
        return test_exec.data_uid, results

    def __init__(
        self,
        benchmark: int = None,
        data_prep: str = None,
        model: str = None,
        evaluator: str = None,
        data_path: str = None,
        labels_path: str = None,
        demo_dataset_url: str = None,
        demo_dataset_hash: str = None,
        data_uid: str = None,
        no_cache: bool = False,
        offline: bool = False,
    ):
        self.benchmark_uid = benchmark
        self.data_prep = data_prep
        self.model = model
        self.evaluator = evaluator
        self.data_path = data_path
        self.labels_path = labels_path
        self.demo_dataset_url = demo_dataset_url
        self.demo_dataset_hash = demo_dataset_hash
        self.data_uid = data_uid
        self.no_cache = no_cache
        self.offline = offline
        self.dataset = None
        self.data_source = None

    def process_benchmark(self):
        """Prepares all parameters so a test can be executed. Paths to cubes are
        transformed to cube uids and benchmark is mocked/obtained.
        """
        if not self.benchmark_uid:
            return

        benchmark = Benchmark.get(self.benchmark_uid, local_only=self.offline)
        if self.data_source != "prepared":
            self.data_prep = self.data_prep or benchmark.data_preparation_mlcube
        self.model = self.model or benchmark.reference_model_mlcube
        self.evaluator = self.evaluator or benchmark.data_evaluator_mlcube
        if self.data_source == "benchmark":
            self.demo_dataset_url = benchmark.demo_dataset_tarball_url
            self.demo_dataset_hash = benchmark.demo_dataset_tarball_hash

    def prepare_cubes(self):
        """Prepares all parameters so a test can be executed. Paths to cubes are
        transformed to cube uids and benchmark is mocked/obtained.
        """

        if self.data_source != "prepared":
            logging.info(f"Establishing the data preparation cube: {self.data_prep}")
            self.data_prep = prepare_cube(self.data_prep)

        logging.info(f"Establishing the model cube: {self.model}")
        self.model = prepare_cube(self.model)
        logging.info(f"Establishing the evaluator cube: {self.evaluator}")
        self.evaluator = prepare_cube(self.evaluator)

        self.model_cube = get_cube(self.model, "Model", local_only=self.offline)
        self.evaluator_cube = get_cube(
            self.evaluator, "Evaluator", local_only=self.offline
        )

    def prepare_dataset(self):
        """Assigns the data_uid used for testing according to the initialization parameters.
        If no data_uid is provided, it will retrieve the demo data and execute the data
        preparation flow.
        """
        logging.info("Establishing data_uid for test execution")
        logging.info("Looking if dataset exists as a prepared dataset")
        if self.data_source != "prepared":
            if self.data_source == "path":
                data_path, labels_path = self.data_path, self.labels_path
            else:
                data_path, labels_path = download_demo_data(
                    self.demo_dataset_url, self.demo_dataset_hash
                )

            self.data_uid = DataPreparation.run(
                None,
                self.data_prep,
                data_path,
                labels_path,
                run_test=True,
                name="demo_data",
                location="local",
            )

        self.dataset = Dataset.get(self.data_uid, local_only=self.offline)

    def initialize_report(self):
        report_data = {
            "demo_dataset_url": self.demo_dataset_url,
            "demo_dataset_hash": self.demo_dataset_hash,
            "data_path": self.data_path,
            "labels_path": self.labels_path,
            "prepared_data_hash": self.data_uid,
            "data_preparation_mlcube": self.data_prep,
            "model": self.model,
            "data_evaluator_mlcube": self.evaluator,
        }
        self.report = TestReport(**report_data)

    def cached_results(self):
        """checks the existance of, and retrieves if possible, the compatibility test
        result. This method is called prior to the test execution.

        Returns:
            (dict|None): None if the result does not exist or if self.no_cache is True,
            otherwise it returns the found result.
        """
        if not self.no_cache:
            return self.report.cached_results()

    def execute(self):
        """Runs the benchmark execution flow given the specified testing parameters
        """
        execution_summary = Execution.run(
            dataset=self.dataset,
            model=self.model_cube,
            evaluator=self.evaluator_cube,
            ignore_model_errors=False,
        )
        return execution_summary["results"]

    def write(self, results):
        self.report.set_results(results)
        self.report.write()
